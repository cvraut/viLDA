library(stats)
library(Rcpp)

## This is an example script currently.

# TODO: 1) turn this into a function that can be exported
# 2) add Rcpp function
# 3) allow for more general mixture components

vocab = c(1,2,3,4,5)
lengthVocab <- length(vocab)
numTopics <- 2
numDocuments <- 32
lengthDocuments <- 24
numWords <- numDocuments*lengthDocuments

alphaWords = 0.2
alphaTopics = 0.2

# initialize word distributions
z1 <- c(17/30,1/3,1/10,0,0)
z2 <- c(0,0,1/10,1/3,17/30)
wordDistributions <- rbind(z1,z2)
generatedTopics <- rep(NA, numDocuments)

docIDs <- c()
words <- c()
currentTopic <- 0

## Assume for now that documents only have one mixture component
## Meaning each document is generated by exactly one topic
## TODO Generalize this to mixtures of topics

for (i in 1:numDocuments) {
  docIDs <- c(docIDs, rep(i, lengthDocuments))
  z = sample(1:numTopics, 1)
  generatedTopics[i] <- z
  dist <- wordDistributions[z,]
  sampledWords <- sample(vocab, lengthDocuments, replace = TRUE, prob = dist)
  words <- c(words, sampledWords)
}

# Data Generation
sampledTopics <- sample(1:numTopics, numWords, replace = TRUE)

Cwt <- matrix(NA, nrow = lengthVocab, ncol = numTopics)
Cdt <- matrix(NA, nrow = numDocuments, ncol = numTopics)

wordsInTopic <- 0
topicsInDoc <- 0

for (w in 1:lengthVocab) {
  for (k in 1:numTopics) {
    wordsInTopic <- words[sampledTopics == k]
    Cwt[w,k] <- sum(wordsInTopic == w)
  }
}

for (d in 1:numDocuments) {
  for (k in 1:numTopics) {
    topicsInDoc <- sampledTopics[docIDs == d]
    Cdt[d,k] <- sum(topicsInDoc == k)
  }
}

numEpochs <- 50000
warmUp <- 5000
lag <- 50
sampleIndex <- 0

listWordParamSamples <- list()
listTopicParamSamples <- list()

probWord <- rep(NA, numTopics)
probDoc <- rep(NA, numTopics)
probTopic <- rep(NA, numTopics)


# Gibbs sampler
for (epoch in 1:numEpochs) {
  for (i in 1:numWords) {
    currentWord <- words[i]
    currentDocID <- docIDs[i]
    Cwt[currentWord, sampledTopics[i]] <- Cwt[currentWord, sampledTopics[i]] - 1
    Cdt[currentDocID, sampledTopics[i]] <- Cdt[currentDocID, sampledTopics[i]] - 1

    for (topic in 1:numTopics) {
      probWord[topic] <- (Cwt[currentWord,topic] + alphaWords) / (sum(Cwt[,topic]) + lengthVocab*alphaWords)
    }

    for (topic in 1:numTopics) {
      probDoc[topic] <- (Cdt[currentDocID,topic] + alphaTopics) / (sum(Cdt[currentDocID,]) + numTopics*alphaTopics)
    }

    probTopic <- probWord*probDoc


    sampledTopics[i] <- sample(1:numTopics, 1, prob = probTopic/sum(probTopic))

    Cwt[currentWord, sampledTopics[i]] <- Cwt[currentWord, sampledTopics[i]] + 1
    Cdt[currentDocID, sampledTopics[i]] <- Cdt[currentDocID, sampledTopics[i]] + 1
  }

  if (epoch > warmUp & epoch %% 25 == 0) {
    wordParameters <- matrix(NA, lengthVocab, numTopics)
    topicParameters <- matrix(NA, numDocuments, numTopics)
    for (topic in 1:numTopics) {
      for (word in 1:lengthVocab) {
        wordParameters[word,topic] <- (Cwt[word,topic] + alphaWords) / (sum(Cwt[,topic]) + lengthVocab*alphaWords)
      }
    }
    for (doc in 1:numDocuments) {
      for (topic in 1:numTopics) {
        topicParameters[doc,topic] <- (Cdt[doc,topic] + alphaTopics) / (sum(Cdt[doc,topic]) + lengthVocab*alphaTopics)
      }
    }

    for (doc in 1:numDocuments) {
      topicParameters[doc,] <- topicParameters[doc,]/sum(topicParameters[doc,])
    }

    sampleIndex <- sampleIndex + 1
    listWordParamSamples[[sampleIndex]] <- wordParameters
    listTopicParamSamples[[sampleIndex]] <- topicParameters
  }
}


# Obtain posterior means
posteriorAlphaWords <- Reduce("+", listWordParamSamples)/length(listWordParamSamples)
posteriorAlphaTopics <- Reduce("+", listTopicParamSamples)/length(listTopicParamSamples)
posteriorAlphaWords
posteriorAlphaTopics







